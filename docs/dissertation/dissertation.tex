\documentclass[12pt]{article}
\usepackage{geometry}
\usepackage{xcolor}
\usepackage[utf8]{inputenc}
\usepackage{tabto}
\usepackage{graphicx}
\usepackage{biblatex}
\usepackage{pdflscape}
\usepackage{hyperref}
\addbibresource{bibliography.bib}
\graphicspath{ {./images/} }
\hypersetup{
colorlinks=false, % TODO 26-Sep-20 Properly configure links https://www.overleaf.com/learn/latex/hyperlinks
linktoc=all
}
\geometry{
a4paper,
left=30mm,
top=30mm,
right=30mm,
bottom=40mm
}
\newcommand{\black}{
\pagecolor{black}
\color{white}
}
\newcommand{\sentence}{} % New sentence
\newcommand{\italic}[1]{\textit{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\title{Vectorization and Compression of Animated Cartoon Videos}
\author{Bassam Helal}
\date{September 2020}
\begin{document}
    % TODO 23-Sep-20 Remove me when the document is full of pictures, a picture is worth 2000 bytes ;)
    % TODO 12-Sep-20 Remove black background and white text
    \black

    \pagenumbering{gobble} % Remove page numbering

    \maketitle

    \begin{center}
        \vspace{8cm}
        \includegraphics[scale=0.65]{SwanseaUniversityLogo.png}
    \end{center}
    \begin{center}
        \vspace{1cm}
        \large
        Bassam Helal
        \linebreak
        809293
    \end{center}

    \pagebreak

    \begin{center}
        \section*{Abstract}
    \end{center}

    % TODO 12-Sep-20 Abstract

    \pagebreak

    \renewcommand*\contentsname{
    \begin{center}
        Table of Contents
    \end{center}}

    \tableofcontents

    \pagebreak

    \pagenumbering{arabic} % Begin page numbering from here


    \section{Introduction}\label{sec:introduction}

    % A paragraph or so for each of:

    % Project Description
    % Motivation
    % Existing Literature
    % Limitations of the literature that we will attempt to tackle
    % Aims and Objectives
    % Discoveries and results we found after our attempts
    % Section Signposting

    \pagebreak


    \section{Background}\label{sec:background}

    \tab
    This document makes no assumption of the reader's knowledge in computer graphics and its related fields,
    thus, this section will introduce the reader to some of the required knowledge needed for understanding this
    project.
    \sentence
    Firstly, we will introduce raster graphics, the current de facto standard for graphics data representation.
    \sentence
    We will then introduce its counterpart vector graphics, a different way to store graphics data that addresses
    some of the issues with raster graphics.
    \sentence
    We will then give a brief primer on image compression and some common methods of reducing the overall size
    of an image.
    \sentence
    Finally, we will explain how image graphics tie into video graphics and describe some of the few differences
    between the two.

    \subsection{Raster Graphics}\label{subsec:raster-graphics}

    \tab
    Raster graphics is the current most popular way of representing and storing graphics data.
    \sentence
    This representation format stores the image data as a grid of pixels, a pixel (picture element) being the
    smallest element of the image containing the color information of the image at a particular point in the grid.
    \sentence
    Colors can be represented in many different formats but the most common is the three channel RGB (Red, Green, Blue)
    with RGBA (Red, Green, Blue, Alpha) gaining popularity.
    \sentence
    Each channel has a bit depth which details the number of possible colors that the channel can contain with 8 bits
    per channel being the current most common but higher values exist as well such as 10 and 12 bits per channel.
    \sentence
    So a pixel in RGB format is really just a 3-tuple (triple) of numbers for each color channel in that pixel.
    \sentence
    An image's resolution is the number of total pixels it contains, which is the image's width multiplied by its
    height, higher resolution images have more detail but are also more expensive to store as there is more data.
    \sentence
    While these concepts are seen as the current norm in computer graphics, they are more specific to raster graphics
    in particular.
    \sentence

    \bigskip
    Raster graphics are the most popular form of image and generally graphics representation as they are easy to
    understand and display.
    \sentence
    The main disadvantages of raster graphics concern image resolution and detail, namely that images will show
    pixelation artifacts when they are displayed at a higher resolution than their own where the image will start
    showing the raw pixels.
    \sentence
    To match the evolving increase in display resolutions, newer media uses higher resolutions in order to show
    optimally on these displays but at a cost.
    \sentence
    The first cost being that the new higher resolution images require more disk space to store as they are larger in
    size, but the less obvious cost is that it leaves the older lower resolution content behind.
    \sentence
    Any content made in low resolutions will forever live with that resolution and continue to scale more poorly as
    displays have higher resolutions.
    \sentence
    Therefore, while raster graphics are the current de facto standard for graphics representation, they are not
    without their major disadvantages, namely concerning the need for increased resolutions, meaning more disk space
    needed and old content becoming more and more outdated.

    % TODO 16-Sep-20 Show pixelation images

    \subsection{Vector Graphics}\label{subsec:vector-graphics}

    \tab
    Another graphics representation format is vector graphics, which addresses some of the issues of raster graphics
    and is growing in popularity within some domains.
    \sentence
    Vector graphics uses a more declarative style of data storage, where instead of storing the data to be shown
    (such as pixels containing color information), vector graphics is based on instructions to a renderer
    about how to draw the image.
    \sentence
    These instructions are based on a set of standard geometric elements that are used in that vector graphics format
    or protocol.
    \sentence
    The most common and well known vector graphics format today is SVG, Scalable Vector Graphics, which is developed
    by the W3C since 1999, this is the format we will be using throughout this document when referencing vector
    graphics.
    \sentence
    An SVG file is nothing more than an XML-like text file with XML elements part of the SVG standard that describe
    to the renderer how to draw the image.
    \sentence
    The SVG specification defines many elements that can be used in an SVG file but the most important one in our
    case is the SVG \code{path} element.
    \sentence
    The \code{path} element can be used to draw a complex line by providing it with commands on where and how to
    draw, it is perhaps the single most powerful element within the SVG specification and is the only element that we
    will use in this project.
    \sentence

    \bigskip
    The greatest advantages of vector graphics that they address the main issues of raster graphics mentioned in
    the~\nameref{subsec:raster-graphics} section, that is to say that vector graphics are resolution independent and
    are (theoretically) infinitely scalable no matter the display.
    \sentence
    The scalability of vector graphics actually solves both issues at once because there is no longer a need for
    higher resolution media thus saving on disk space \italic{and} keeping old media still usable because of infinite
    scalability.
    \sentence
    But such advantages don't come with some issues, and there's a reason vector graphics are not the current norm.
    \sentence
    Simply put, vector graphics are only good at displaying certain kinds of images and media because of the way they
    represent data, making it very difficult to accurately and efficiently store other kinds of images such as images
    of natural scenery or faces.
    \sentence
    This is because those kinds of images tend to have a large degree of color fluctuations and especially
    non-linear, complex color fluctuations.
    \sentence
    This is a particular weakness of vector graphics and one that has limited its adoption and growth because
    compared to raster graphics in this regard, vector graphics are significantly inferior.
    \sentence
    As a result, vector graphics are seen as mostly domain and use-case specific and not general purpose, they are
    excellent for logos and icons specifically because of their scalable nature but are inferior when used for
    photographs because of the complex color fluctuations of those images.

    % TODO 16-Sep-20 Small table comparing Raster & Vector Graphics for the attributes like size,scalability etc

    % TODO 23-Sep-20 Images with Raster and Vector side by side, 1 for logo (Vector winner) 1 for portrait (raster winner)

    \subsection{Image Compression}\label{subsec:image-compression}

    % Brief summary of what we will go over (signposting)
    % Fundamentals of Compression (type-agnostic) things like compression rate and lossy-ness or accuracy
    % Lossless Image Compression (PNG, BMP etc)
    % Lossy Image Compression (JPEG, GIF etc)
    % Brief summary of everything we just mentioned

    % TODO 16-Sep-20 We can have an image comparing popular image file types with regards to compression

    \subsection{Vectorization}\label{subsec:vectorization}

    \tab
    Converting graphics from raster to vector allows for one to make use of the benefits of vector graphics and is a
    used by people across multiple domains, this process is known as vectorization or raster-to-vector conversion.
    \sentence
    This process is often used by specialists when viewing image data captured in raster to be viewed by a vector
    graphics program that can then perform additional processing such as classification, labelling etc.
    \sentence
    This process is, by its nature, lossy, meaning the original data will be lost during the process because the
    process is transformative and even though a lossless process is theoretically possible, it would not be practical
    or efficient as it would not introduce any of the benefits of vector graphics and would instead just be
    converting pixels into pixels in vector graphics.
    \sentence
    Therefore, the process aims to be \italic{Visually Lossless} aiming for highest accuracy while also trying to
    effectively and efficiently transform the data into suitable optimal vector graphics data.
    \sentence
    This will often mean trying to undo rasterization effects or artifacts in order to achieve a better output that
    makes sense in a vector format such as for example, interpolating pixels in order to create smooth lines
    or doing some kind of color reduction, both of which change the original data in order for it to make more sense
    in the vector output.

    \bigskip
    \sentence
    In that vain, it is also important to note that the vectorization process will also be dependent on the input
    data provided with images that do not have vectorizable qualities resulting in poor vector results, either in
    terms of accuracy or file size or both.
    \sentence
    These kind of images are most often photographs or other kinds of images with a high degree of color fluctuations
    as mentioned in the~\nameref{subsec:vector-graphics} section.
    \sentence
    Whereas images with discrete color regions such as logos, icons and other "man-made graphics" often contain less
    of these qualities and will more likely do well in the vectorization process because the vectorization process
    does not work well on color fluctuations.
    \sentence
    In such cases raster graphics is always a better format as it will much more effectively store such data with
    high clarity and lower file sizes, as such, it is important for one to know where each format is more suitable as
    they each have their advantages and disadvantages.
    \sentence
    Knowing that however, when one does deduce that vector graphics is the more suitable format for their graphics,
    vectorization will allow those people to convert their graphics from raster to vector in a manner that is as
    effective as possible in order to obtain visually lossless transformation that will still be optimal as a vector
    graphics output.

    \subsection{Video Graphics}\label{subsec:video-graphics}

    % Brief summary of what we will go over (signposting)
    % How video is stored and how it relates to Images (mostly the same)
    % Common file types and how they compress and store video data (MP4, MKV, H265 etc) MPEG1 temporal redundancy
    % Evidence and proof of why video is expensive (and also popular)
    % Brief summary of everything we just mentioned

    \subsection{Toolchain}\label{subsec:toolchain}

    \tab
    This project uses a variety of popular tools and libraries to achieve its aims and objectives both efficiently
    and quickly, this section will briefly go over the most important tools used.
    \sentence
    This project uses Node.js as its runtime platform, Node.js is a cross-platform runtime environment that executes
    JavaScript code on a machine instead of the usual JavaScript runtime, the browser.
    \sentence
    The choice for Node.js was made paradoxically because of both a familiarity with the runtime and the JavaScript
    ecosystem as well as a strong desire to further enhance our knowledge and experience in both the platform and
    ecosystem.
    \sentence
    Node.js uses JavaScript as its runtime language, while JavaScript is a powerful and excellent language for
    rapid-prototyping, it is also a very error-prone language as it has no typing checks in place.
    \sentence
    TypeScript is a programming language that is a strict superset of JavaScript that adds typing features onto
    JavaScript and transpiles to performant, cross-platform compatible JavaScript by means of a TypeScript compiler.
    \sentence
    TypeScript was chosen as the programming language of choice for the project for its many benefits not limited to
    its added type safety.
    \sentence

    \bigskip
    \sentence
    Node.js also allows us to access the ever-growing JavaScript ecosystem which includes libraries and
    frameworks for both the front-end and back-end side of development, of which this project makes use of some
    notable tools such as Electron, GPU.js and OpenCV among others.
    \sentence
    Electron allows developers to write desktop applications using web technologies, it is essentially a Chromium
    browser with two processes, a front-end process called the "renderer" which is similar to a Chromium web page
    that displays HTML, and a back-end process called the main process which is a Node.js process.
    \sentence
    Electron combines Node.js and Chromium to allow a developer to write a native desktop application using the same
    technologies used on the web, this has its own advantages and disadvantages but in our case, it means we can
    quickly develop a GUI desktop application that can access our Node.js code without resorting to web servers and
    other more complicated means.
    \sentence
    GPU.js is a JavaScript library that allows developers to run code on the host machine's GPU making use of the
    GPGPU (General-Purpose computing on Graphics Processing Units) paradigm to allow for very large performance
    improvements and speedups for computationally expensive mathematical operations.
    \sentence
    In summary, the project uses the Node.js runtime for its large ecosystem, the TypeScript programming language for
    its added type safety benefits, Electron to easily display a GUI and GPU.js to run code on the GPU for additional
    performance when needed.
    \pagebreak


    \section{Related Work}\label{sec:related-work}

    % Here we throw a lot of the current stuff from both research and industry and show their flaws and limitations
    % (which we may or may not have tackled)

    \subsection{Current Research}\label{subsec:current-research}

    % Brief summary of what we will go over (signposting)
    % New Machine Learning methods for Vectorization
    % New Machine Learning methods for Compression
    % Novel approaches and ideas within Vectorization such as mesh gradients etc (no ML)
    % Brief summary of everything we just mentioned

    \subsection{Current Applications}\label{subsec:current-applications}

    % Brief summary of what we will go over (signposting)
    % Adobe Illustrator, Logos, Topography, Medical stuff like X rays etc Potrace
    % Animations and Games using Vector as the development method
    % Current Image \& Video compression algorithms such as PNG, H265 and H266 (no ML)
    % Brief summary of everything we just mentioned

    \subsection{Existing Limitations}\label{subsec:existing-limitations}

    % Brief summary of what we will go over (signposting)
    % Why ML is absolute hot trash
    % Show off the flaws with existing technologies while showing how we tackled (and hopefully beat) them
    % Discuss inherent limitations of this process, such as issues with Vector Graphics and input data being varying etc
    % Brief summary of everything we just mentioned

    \pagebreak


    \section{Implementation}\label{sec:implementation}

    \tab
    The following section will go into high level detail describing the implementation details of the project.
    \sentence
    While this section may seem rather long, it is in fact made dense to ensure conciseness while keeping all the
    necessary details explained.
    \sentence
    This is because the project explores many different ideas and implementations to try to achieve its goals as will
    be evident by the multiple vectorization methods used.
    \sentence
    Firstly, we introduce the high level process pipeline of the project from input to output.
    \sentence
    Then, we explore in depth some of the many methods of vectorization used in the project.
    \sentence
    Finally, we describe the limitations and drawbacks that were found during the implementation.

    \subsection{Pipeline}\label{subsec:pipeline}

    \bigskip

    % TODO 25-Sep-20 Make pipeline image look better

    \includegraphics[width=\textwidth]{Pipeline.png}

    % TODO 15-Sep-20 Add a caption, read more here https://cs.overleaf.com/learn/latex/Inserting_Images

    \bigskip

    The above image shows the high level process pipeline of the project from input to output with each phase of the
    pipeline further detailed in its own section below.
    \sentence
    Firstly, we explain the expected input video and its ideal characteristics that will result in a higher accuracy
    output.
    \sentence
    Then, we establish the expected and desired output and detail its expected and ideal characteristics before
    further discussing any of the processing.
    \sentence
    Then, the first phase of the processing begins, the frame extraction, where the frames of the video are
    extracted to be used further down the pipeline.
    \sentence
    Next, the main phase of the processing, the vectorization of a single frame, we describe and detail the multiple methods
    and approaches that were used in order to gain the highest level of accuracy and compression rate on each frame.
    \sentence
    Finally, the final stage of the processing, frame merging combines the vectorized frames back into a playable
    video form with additional computation executed to add favorable traits such as further reduced file sizes.

    \subsubsection{Input}\label{subsubsec:input}

    \tab
    Firstly, it is important to understand the input that the process will be expecting and detail what makes an ideal
    input file that will produce the best results.
    \sentence
    The minimum requirement is that the input file be any video, but more specifically, this project
    aims to work on animated cartoons, so while any video will technically work, for the purpose of this project, a
    video of an animated cartoon is expected.
    \sentence
    However, not all animated cartoons are equal, in fact there is a great deal of variation between cartoons in
    terms of art style, animation method, and many other artistic effects that can heavily influence the output of
    the process.
    \sentence
    Thus, we must define the "ideal" input file, or more precisely, the input file that will produce the best results
    because the process was designed with those files in mind for reasons to do with the limitations of vectorization.
    \sentence
    The ideal input video is one that is most friendly towards vectorization, meaning it has the most vectorizable
    qualities and characteristics with as few incompatibilities as possible, these were mentioned in depth
    in the~\nameref{subsec:existing-limitations} section but will be briefly repeated within this new context.

    \bigskip
    Since vectorization is heavily dependent on the color variation of the original raster image, an ideal input
    image would be one with discrete color regions made up of a single color, with no noise, gradients, shadows,
    lighting or any other effects, essentially an image made of a set of color blocks that are very discrete from one
    another.
    \sentence
    In addition to this, it is important to note that we are dealing with moving pictures and it is also ideal to
    have as little noise as possible \italic{between} frames, this can be an artifact of compression or
    rasterization but can also be seen as an artistic style choice as shown in Cartoon Network's
    \italic{Ed, Edd n Eddy}.
    \sentence
    Obviously very few real world media satisfies these conditions, but a surprising number of cartoon videos can come
    very close to this description, with very few complications such as shadows and light gradients that are usually
    in the background or are less important elements of the image.
    \sentence
    An example of a real world cartoon that has these ideal qualities is Nickelodeon's \italic{The Fairly OddParents}
    which uses discrete color regions, very few (if any) gradients and no complex shadows or lighting.
    \sentence
    On the other extreme end cartoons with heavy use of photographs or heavy noise are the least ideal as well as
    any 3D cartoons, ie those that make use of 3D effects such as shadows, lighting and many complex gradients.
    \sentence
    Examples include Hanna Barbera cartoons prominent from the 1960s to the 1980s such as \italic{Top Cat}
    and\italic{The Flinstones} and modern 3D animations such as Pixar Animation Studios'
    \italic{Toy Story} and \italic{The Incredibles}.
    \sentence
    In summary, the minimum required input is simply any video but the expectation is that it will be an
    animated cartoon video and the ideal input being an animated cartoon video that has highly vectorizable qualities.

    % TODO 20-Sep-20 Show examples of good cartoons and have images, so Fairly OddParents, Teen Titans Go
    % TODO 20-Sep-20 Show example of some less optimal cartoons like old Tom and Jerry, Hannah Barbera and Ed, Edd n Eddy

    \subsubsection{Output}\label{subsubsec:output}

    \tab
    Before discussing any of the processing, it is important to first understand the expected output that the process
    will generate and detail the expected and ideal characteristics of the output.
    \sentence
    In essence, the output is a playable video that uses vector graphics to store the same data as the input file but
    with the added benefits of vector graphics, namely improved scalability and reduced size.
    \sentence
    Ideally, the output will be visually indistinguishable from the original but with a smaller size essentially
    achieving so called~\italic{Visually Lossless Compression}.
    \sentence
    However, since the video is using vector graphics, it will be much more scalable in comparison and essentially
    void of the pixelation problem that plagues traditional raster graphics.
    \sentence
    The size difference should especially be evident when taking the upscaling ability into account since the new
    video can be effective at any resolution display whilst the original will show pixelation once it is displayed on
    high enough resolution displays.
    \sentence
    Thus, an input video in 480p could likely produce an output video of larger size, and an input video in 720p
    could likely produce an output of similar size to the original, but the new vector graphics videos will be
    scalable to any resolution such as 4K or higher where the originals will show major pixelation at those resolutions.
    \sentence
    One important thing to note is that the output will still be frame based, this means the video data is frames of
    images moving at some given framerate, as is the case with raster video formats.
    \sentence
    This is in contrast to what vector animations are often represented in (usually for UI animations) and that is
    using some form of interpolation making the playback framerate independent, that is not the case here as that
    would add much more complexity.
    \sentence
    This frame-based characteristic of the output is important because it means we can leverage existing theories and
    concepts from raster videos mainly with regard to compression as is made clearer in
    the~\nameref{subsubsec:frame-merging} section.
    \sentence
    This frame based vector graphics video output makes the process as simple as possible while still achieving the
    desired benefits of vector graphics into the original raster video, namely improved scalability and reduced size.

    \subsubsection{Frame Extraction}\label{subsubsec:frame-extraction}

    \tab
    The frame extraction phase has a single purpose and that is to convert the input video into a some form of medium
    that can be vectorized, namely images.
    \sentence
    Since the input video will always be frame based, we can thus convert a video into a set of vectorizable units
    that accurately represent its data, frames.
    \sentence
    Each frame of the video will be represented in an image which we can then vectorize in the next step.
    \sentence
    To do this, we use the industry standard tool, FFmpeg to create a directory of PNG images for each frame of the
    video using the command:
    % TODO 25-Sep-20 Possibly explain the command
    \begin{center}
        \code{ffmpeg -i input.mp4 outputDirectory/\%d.png -y}
    \end{center}
    \sentence
    This is so that the next step (vectorization) can be done on each frame can be independently and can more easily
    be parallelized, since the vectorization of a single frame does not depend on any other frames around it.
    \sentence
    While in theory one can skip this step and read each frame of the video performing the vectorization step in
    serial for each frame, it is much more efficient to split the workload first by first dividing the video into
    frames stored on disk and then perform the vectorization step, now with the additional benefit of being able to
    run the vectorization step on multiple frames at once.

    % TODO 20-Sep-20 Picture showing the directory of frames

    \subsubsection{Vectorization}\label{subsubsec:vectorization}

    \tab
    The most important and most difficult step of the process is vectorization, which converts
    frames from raster graphics to SVG vector graphics.
    \sentence
    The expected behavior of this step is extremely simple, a raster graphics input image will be converted to a
    vector graphics output image ideally one with the highest possible accuracy and lowest size possible.
    \sentence
    While the expected behavior is extremely simple, the actual process is extremely difficult and complicated and
    there are many possible correct approaches to this problem that yield varying results.
    \sentence
    These processes are detailed more thoroughly in the~\nameref{subsec:vectorization-methods} subsection further below
    with their advantages and disadvantages.
    \sentence
    From a high level standpoint however, all the approaches aim to achieve the same result, that is to transform the
    data from one representation format (raster) to another representation format (vector).
    \sentence
    This inherently means that there will be some form of data loss with the goal being to achieve what is called
    \italic{Visually Lossless Compression} while gaining the benefits of vector graphics such as scalability in
    addition to the aforementioned smaller size.
    \sentence
    To achieve this, we convert the input image into an SVG image made up only of one SVG element, the powerful
    \code{path} element.
    \sentence
    As mentioned earlier the \code{path} element allows us to declare a path of arbitrary shape by declaring the
    commands used to draw it.
    \sentence
    An image can be represented as a set of discrete connected color regions that together form the whole image,
    these can be done using the path element which can then be filled with a single color allowing us to create
    single-color regions of arbitrary shape.
    \sentence
    Across all tried approaches, this is the common method used to create a vector graphics image, with the only
    differences being in how these color regions are parsed and determined from the original raster image.

    % TODO 25-Sep-20 Picture showing pixels of a single color being converted to a single SVG path

    \subsubsection{Frame Merging}\label{subsubsec:frame-merging}

    \tab
    The final processing step is frame merging, the step that will combine all frames (now in glorious vector
    graphics) back into some playable video form.
    \sentence
    This is the step where the most optimizations and improvements can be made with regards to file size.
    \sentence
    Since we are returning back to the realm of video, i.e. moving pictures, we can make use of the existing
    compression technology and theory that is used in raster based video to drastically reduce file size.
    \sentence
    This is true because the output is still frame based just as raster videos are, and thus the same theories can be
    applied, this would not be true if the video was not frame based such as using interpolation or some other
    advanced method.
    \sentence
    The main concept to be brought over from current raster video compression is compression through leveraging
    temporal redundancy in a video as mentioned in the~\nameref{subsec:video-graphics} section.
    \sentence
    In this case, we can join a series of similar frames into what we call a frameset which will have a root
    or anchor frame.
    \sentence
    Only the root frame will be completely stored, all successive frames in the frameset will only be stored as the
    delta between the root frame.
    \sentence
    This is currently the only concept that is taken from raster video to reduce size but as mentioned, since the
    video is still frame based, many of the same ideas can be directly ported into the vector graphics world for
    further size reduction.
    \sentence
    Many more compression optimizations can be done during this step to further reduce the size of the final
    video, hence the importance of the frame merging step within the process pipeline.

    % TODO 25-Sep-20 Picture showing too many successive frames are identical

    \subsection{Vectorization Methods}\label{subsec:vectorization-methods}

    \tab
    As previously mentioned, many different approaches and methods were used to tackle the vectorization step of the
    pipeline in order to achieve the best possible results, or sometimes even acceptable results in some cases as
    will be detailed below.
    \sentence
    Three main approaches were used to tackle the problem, a color quantization based approach and two connected
    component labelling based approaches, one using edges as components and the other using colors as components.
    \sentence
    One detail to note is that multiple approaches were not planned early on and were in fact researched and explored
    as the project developed and as results began to appear.
    \sentence
    This is noteworthy because each successive approach aimed to fix the issues with the previous one, only to
    introduce new issues of its own.
    \sentence
    We will explain all three approaches in depth in their own subsections below as well as give an introduction to
    the theory of connected component labelling as two of the three approaches are based on this.

    \subsubsection{Color Quantization Approach}\label{subsubsec:color-quantization-approach}

    \tab
    The first approach is based on color quantization, or simply, reducing the total number of distinct
    colors in an image while still aiming for \italic{Visually Lossless Compression}.
    \sentence
    This approach was chosen because it is the one used in the \italic{ImageTracer.js} library that
    was used as an initial blueprint and foundation for the project's codebase.
    \sentence
    Color quantization can be implemented in a variety of different algorithms but essentially any 3-dimensional (or
    4-dimensional if including alpha) clustering algorithm can work because it is in essence a classification problem.
    \sentence
    All algorithms have the same expected behavior though;
    given an image and a desired number of colors \italic{n}, find the \italic{n}-size set of colors that best
    represents this image.
    \sentence
    Each pixel in the image is then changed from its original color to the color in the reduced palette that is
    closest to its original color, and thus the image has been reduced since the image has only \italic{n} distinct
    colors.
    \sentence
    Each distinct color can then be assigned a single SVG path element, this is like a layer because it need not
    necessarily be connected, thus we end up with an SVG image made up of \italic{n} number of SVG path elements,
    each a single color.

    \bigskip
    Color quantization is an excellent way to reduce the size of an image drastically while making it visually
    similar to the original and it is usually an extremely fast and efficient process.
    \sentence
    Their are a few drawbacks though, the first being that it performs much worse when attempting to reduce images
    with gradients and noise and there will be a visually obvious loss of data, similar to the issues that affect
    vectorization.
    \sentence
    However, in our case, the larger issue is to do with the manually inputted number of colors that the algorithm
    requires.
    \sentence
    The number of colors that are given to a color quantization algorithm can drastically change all of accuracy,
    performance and file size all at the same time.
    \sentence
    A high number such as 1,000 colors will yield very high accuracy for most images but will have much less size
    reductions and have significant performance impacts.
    \sentence
    Even ignoring performance, choosing the correct number of colors that will yield the highest compression rate while
    also being visually identical in quality is another major task in itself, especially when considering that not
    all frames or images have the same color space.
    \sentence
    The truth is, while color quantization is an excellent way to reduce file size and still achieve
    \italic{Visually Lossless Compression}, it depends too heavily on the provided number of colors and
    computing this color deterministically is another major problem of its own.

    \subsubsection{Connected Component Labelling (CCL)}\label{subsubsec:connected-component-labelling-(ccl)}

    \tab
    A more logical way to approach the problem is to approach it as a partitioning problem, meaning we want to
    partition the image into a set of discrete adjacent parts that altogether make up the whole image.
    \sentence
    This is because that is exactly how the end result SVG will be created, as a set of SVG path elements that together
    form the whole image as if they are parts of a partition.
    \sentence
    This way of thinking lends itself extremely well to Connected Component Labelling (CCL) which is exactly that,
    dividing the image into a partition of connected components that together form the whole image.
    \sentence
    % TODO 26-Sep-20 Where CCL is used and some theory background
    \sentence
    Connected Component Labelling does not in itself describe any particular implementation and thus can be
    implemented in many different ways depending on the context and desired outcome.
    \sentence
    However, the basic idea is that a single pass is made across the image and each pixel is checked to see if it
    fits within its neighbors' components, if so it is added to that component, if not it is the first in a new
    component.
    \sentence
    A second pass may be made to merge any components that are adjacent but this can be done in a single pass
    by checking to merge components after the initial component assignment.
    \sentence
    The concept itself is relatively simple with the intricate details left to the implementation which can heavily
    affect the results depending on how it is implemented and approached.
    \sentence
    Two CCL based approaches were used to attempt to tackle the vectorization problem, each differing only in the
    means by which a "component" is defined, the first using edges as components and the second using pixel colors as
    components.

    \subsubsection{Edge Based CCL Approach}\label{subsubsec:edge-based-ccl-approach}

    \tab
    The first approach using CCL is based on defining a component as an edge of the image.
    \sentence
    Edges can be easily detected using any edge detection algorithm, for this we used OpenCV's Canny edge detection
    but the actual edge detection is not the important factor here.
    \sentence
    For higher accuracy and confidence, a large sample size of edge detection passes with different parameters was
    averaged to create a single averaged edges image.
    \sentence
    This image is a monochrome image in which, a pixel with value of 0 is never an edge in any pass and a pixel with
    a value of 255 is an edge in every pass, thus we need to use some thresholding to determine what defines an edge.
    \sentence
    Once that is determined however, the CCL algorithm can pass through the average edges image to find the connected
    edges that make up the image.
    \sentence
    The reasoning behind this is that in theory, this will return the edges of all the polygons that make up the
    image and thus by filling them with their average color, we can recreate the image using color regions.
    \sentence
    However, this is much easier said than done, and indeed there are many edge cases that need to be noted,
    specifically the effect of a single pixel.
    \sentence
    A single missing pixel could mean a polygon is incomplete, a single extra pixel between two unrelated polygons
    could mean the polygon is incorrect, basically the algorithm is very sensitive to any discrepancies within the
    averaged image.
    \sentence
    This can be tackled using further processing and analysis to undo some of these effects, but this would be very
    complicated and time consuming.
    \sentence
    In the end however, the edge based CCL approach did not yield the expected results and was very disappointing,
    although theoretically it can be perfected with time, that was something that was very limited and valuable and
    thus a new approach was explored.

    \subsubsection{Color Based CCL Approach}\label{subsubsec:color-based-ccl-approach}

    % Using the color of each pixel to form regions with close enough or similar colors

    % Explain pixel data (RGBA) and what it means to be close to a neighboring pixel
    % Reasoning behind approach (an image can be a partition of several color regions)
    % Advantages (surprisingly accurate, little data loss, huge size savings)
    % Disadvantages (slow, complex, Anti-Aliasing and noise affects result so too many 1 pixel regions)

    \tab
    The second approach using CCL is based on defining a component as a region of similar colors based on the pixel
    values.
    \sentence
    This approach is the most logical way to approach the problem solely based on the desired output, i.e., SVG path
    elements of a single color each representing a color region.
    \sentence
    Since each pixel contains its color information, we can cluster pixels that are adjacent and have colors that are
    within a given delta threshold, that is to say, they are close enough in color to be part of the same color
    region.
    \sentence
    In essence we are performing a kind of color quantization since there will be reduced colors at the end of the
    process but unlike classic color quantization mentioned in the~\nameref{subsubsec:color-quantization-approach}
    section, the desired number of colors is no longer an input, solving the issue with that approach.
    \sentence
    As with other CCL approaches, the image is looped on and each pixel is queried to check whether it is eligible to
    be part of any of the components of its preceding neighbors, if so it joins that component, if not it is assigned
    its own new component where it is the first element.
    \sentence
    The reasoning behind this approach is that in theory each color region in the original image will naturally form
    a cluster which can then be reduced to a single color that is an average of all the pixels in that region.
    \sentence
    Thus, the image is transformed from a grid of unrelated pixels to a partition of adjacent color regions each
    being a single color that altogether form the entire image.
    \sentence

    \bigskip
    While being fairly complex and computationally expensive, this approach yields shockingly positive results both
    in terms of size reduction and image accuracy with any data loss incurred being minor.
    \sentence
    However, upon further inspection, a huge issue became obvious;
    the total number of components that was created was surprisingly high, an image with at most perhaps a few
    thousand color regions results with tens of thousands of color regions.
    \sentence
    The distinct colors were still reduced so color quantization was successful but an efficient partitioning was not
    and upon further investigation and research, it was clear that the culprit was anti-aliasing.
    \sentence
    Anti-aliasing is an effect popular in raster graphics that is used to make edges less sharp by adding blurs
    between them, and in the case of two different color regions, pixels with colors \italic{in between} those two
    colors.
    \sentence
    What ends up happening is that our algorithm cannot determine which region these \italic{in between} pixels are
    because their colors are not close enough to either region and so they are assigned their own region.
    \sentence
    This is shown when we analyze the number of components with less than 5 total pixels and the result is a whopping
    97\% of all regions have less than 5 pixels in total and upon investigating, it is clear that these are the
    pixels at the edges of color changes and thus the ones responsible for anti-aliasing.
    \sentence

    \pagebreak


    \section{Software Engineering}\label{sec:software-engineering}

    \tab
    This section will detail the software engineering aspects of the project's development, specifically the
    methodology used to develop, the risks of the project and the software testing techniques used in the project's
    development.

    % Just the SE stuff and how we applied it during development, briefly mention the changes Covid did to the development

    \subsection{Methodology}\label{subsec:methodology}

    % Extreme Lean Agile because life is unknown beyond 2 days in these strange times we live in :/
    % Here we can show how the poor results affected our schedule and how things shifted because of unexpected results,
    % we can definitely show that this is common in real life and is a good representation and preparation of the real world
    % which has unexpected delays and poor results all the time and how we overcame it all (less sleep and more coffee :D)

    \tab
    The software methodology used for the development of this project is a very lean agile methodology aptly named,
    Lean-Agile.
    \sentence
    % TODO 20-Sep-20 Ref https://www.bluefruit.co.uk/lean-agile-series/whats-lean-agile-how-can-help-you-avoid-project-waste/
    This methodology inherits heavily from modern agile methodologies following all the core principles and values
    but adds principles adapted from lean manufacturing, particularly emphasizing minimal waste and minimal
    prediction.
    \sentence
    While a more commonly known methodology such as Scrum could have been used, it seemed more appropriate to use a
    methodology that has extreme unpredictability built into it nature especially during the growth phase of the
    COVID-19 pandemic, which caused many unpredictable events.
    \sentence
    The pandemic's potentially adverse effects on the development of the project meant that it was critical to use a
    methodology that is extremely efficient and has little fixed planning with a very easy way of adapting to changes.
    \sentence
    This proved to be extremely beneficial, more so than expected because of the many major roadblocks that were
    faced during development which are further explained in detail in the~\nameref{sec:results} section.
    \sentence
    The constant occurrence of very negative results, especially after a complex and time-consuming amount of work
    put in meant that plans had to change and adapt rapidly in accordance to this, which is common and expected in
    any research based project such as this one.
    \sentence
    As a result, while the Lean-Agile methodology is an unorthodox methodology choice, it was absolutely the correct
    one to use and has been the saving grace of the time management of this project since very few other
    methodologies could have allowed for the rapid adaptation and movement of this project.

    \subsection{Schedule}\label{subsec:schedule}

    % Followed it well but started falling apart towards the end, very accurate to the real world :D
    % Mention that we started dissertation somewhat early to avoid the risk of running out of time

    \tab
    While the Lean-Agile methodology is not particularly well suited to long term predictions, it is still important
    to have a schedule in place in order to keep track of time and to have a general idea of how many tasks are left
    and how much time is available to complete them, even if the predictions of time may be incorrect.
    \sentence
    This is why a Gantt Chart was used to plot a schedule containing the general phases of the project's development
    against the time allocated to work on the project.
    \sentence
    % TODO 21-Sep-20 Describe what is a Gantt Chart
    \sentence
    % TODO 21-Sep-20 Introduce the Gantt chart below


    \pagebreak
    \thispagestyle{empty}
    \begin{landscape}
        \begin{center}
            \huge{Gantt Chart}
            \normalsize
            \begin{itemize}
                \item Frame Managing (video to frames and frames to video)
                \subitem Video to frames (ffmpeg)
                \subitem Read frame image data
                \subitem Write frame image data (SVG and PNG)
                \subitem Combine frames to a playable video
                \item Vectorization
                \subitem Core programming (ImageTracerJS)
                \subitem Edge detection
                \subitem Connected Component Labelling
                \subitem Path/Polygon parsing
                \item Writing
                \subitem Latex setup and learning
                \subitem User evaluations
                \subitem Resource compiling
                \subitem Core writing
            \end{itemize}
        \end{center}
    \end{landscape}
    \pagebreak

    % TODO 21-Sep-20 Detail the stuff we showed in the chart above

    \pagebreak

    \subsection{Risks}\label{subsec:risks}

    \tab
    Naturally, a project of this scale that is very experimental in nature has many multi-faceted risks
    associated with it that can severely impact the development of the project if not correctly mitigated.
    \sentence
    As such, it is important to detect these risks as early as possible and have detailed and usable mitigation
    strategies to counter such risks and avoid their possibly devastating effects.
    \sentence
    % TODO 21-Sep-20 Introduce the chart below and describe the format of the chart
    % TODO 21-Sep-20 Chart should have Risk name, description, type? Probability, Consequentiality, mitigation

    \pagebreak
    \thispagestyle{empty}
    \begin{landscape}
        \begin{center}
            \huge{Risks Chart}
            \normalsize
            \begin{itemize}
                \item Bad time management
                \item Data Loss
                \item Hardware performance issues
                \item Tools too difficult
                \item Tools not cooperative
                \item Personal issues (stress, burnout)
                \item Covid related crap
                \item Poor results
                \item Catastrophic fatal disaster ;)
            \end{itemize}
        \end{center}
    \end{landscape}
    \pagebreak

    \subsection{Testing}\label{subsec:testing}

    \tab
    Testing is an important part of software engineering, as it allows developers to ensure that the program behaves
    as expected even after changes to the code, this means that refactors or new implementations can easily be done
    since any breaking changes will fail the tests that previously passed.
    \sentence
    This makes developing new changes or code more reliable since there is a pillar of trust knowing that if the
    tests pass, then the software is behaving correctly, assuming of course the tests are well written and defined.
    \sentence
    In order to ensure that the application is behaving in the expected way, the project needed some kind of
    testing facilitation.
    \sentence
    A variety of classic testing methodologies and philosophies were used in this project namely, unit testing,
    integration testing and acceptance testing.

    \subsubsection{Unit Testing}\label{subsubsec:unit-testing}

    \tab
    The first testing methodology used was none other than the classic and simple unit testing.
    \sentence
    In our case we used a variety of classic automated unit tests, in-code \code{assert} statements and regular
    logging messages each serving different purposes.
    \sentence
    Firstly, the typical automated unit tests were used at the end of a computation to ensure that the output image
    was as expected, using whatever possible methods to do this.
    \sentence
    One of the ways was to test the output image's properties such as resolution, bit depth, size, distinct colors,
    among others.
    \sentence
    Secondly, in-code \code{assert} statements were used before any requirement that was needed to be true before
    computation, usually with regards to expected inputs.
    \sentence
    While this is not a traditional testing method, it was used as such here as there are many times where the input
    is not guaranteed to meet the requirements and thus a graceful failure is the only option with detailed messages.
    \sentence
    Finally, regular logging messages were used throughout the processing.
    \sentence
    These were used less as hard tests and more as indicators of less critical correct or incorrect behavior and very
    often used to measure time passed for every phase or step as execution times became fairly long.
    \sentence
    These three techniques were all used to ensure that the application was tested from the lowest point possible and
    the developer is able to understand how the code is being run and when and why it may be failing.

    \subsubsection{Integration Testing}\label{subsubsec:integration-testing}

    \tab
    Since many phases and process had to interact with one another and pass data to each other, it was important to
    ensure that this was behaving correctly using integration testing.
    \sentence
    In this case, a process such as average edge detection for example would have a specific output that perhaps the
    next process, edge based CCL, may not be expecting or may not know how to behave with.
    \sentence
    In addition to the core concepts were used from unit testing such as using in-code \code{assert} statements
    and logging statements, additional code was added in order to ensure that the application would either use the
    same data formats, or have a trustworthy converter to convert the data.
    \sentence
    An excellent is example is OpenCV's \code{Mat} type representing a matrix which was used during edge detection
    but the next steps would only know how to handle a built in type representing images.
    \sentence
    Thus, a converter was created to ensure that whenever the \code{Mat} type was used it was immediately converted
    to the internal type ensuring that the entire application would be uniform in what data types it would use and
    expect.
    \sentence
    Using integration testing using measures like these, we can ensure that the application can have components and
    processes interact with one another in the expected and correct behavior.

    \subsubsection{Acceptance Testing}\label{subsubsec:acceptance-testing}

    \tab
    Perhaps the most used and most important methodology used in this project was acceptance testing.
    \sentence
    In this case specifically this was done manually while developing in order to ensure results are visually as
    expected.
    \sentence
    This is because this is a graphics based project after all and the final judge as to whether the image is correct
    or not, is of course the human eye.
    \sentence
    A typical test case here was to run some computation on a frame, (have a coffee because it took a while) and
    check to see if the output image was something that was at least close to what was expected.
    \sentence
    This was how the majority of the main important validation was done since no automated test can ever tell if an
    image is acceptable.
    \sentence
    Indeed, there were many cases where the output image would have the expected properties such as resolution, color
    depth, size reduction etc and yet still be nothing more than noise because of some oversight or bug in the code.
    \sentence
    Thus, it is safe to say that while unit testing and integration testing were extremely important in this project,
    the true most important testing methodology used in this project was that which uses the the human eye and that
    being acceptance testing.

    \pagebreak


    \section{Evaluation}\label{sec:evaluation}

    \tab
    In order to know when the program is behaving correctly and to evaluate how well it is performing its task, we
    need to define some evaluation methods and metrics.
    \sentence
    Essentially there really only exist two key traits to the process, how accurate is the process and how well is
    the compression.
    \sentence
    A possible third angle could be performance, but this was never a priority to begin with and this can be heavily
    optimized and improved later, not to mention that it will differ across machines and architectures.
    \sentence
    Thus we will only discuss and detail the two key evaluation properties, accuracy and compression.

    \subsection{Accuracy}\label{subsec:accuracy}

    \tab
    The first property to evaluate is accuracy, or more accurately (pun intended), the lossy-ness that is incurred
    from the process.
    \sentence
    As mentioned earlier, since the vectorization process is transforming data from one form of representation to
    another, there will be data loss, especially if one intends to reduce size as we do.
    \sentence
    However, since we are aiming for \italic{Visually Lossless Compression}, it is important to know how to measure
    this data lossy-ness accurately in a meaningful way.
    \sentence
    Below we detail the two methods of evaluating the accuracy of an image using quantitative methods and qualitative
    methods.

    \subsubsection{Quantitative Methods}\label{subsubsec:quantitative-methods}

    \tab
    The quantitative methods we can use to evaluate the accuracy of the process are fairly straightforward since
    really all that we need to do is compare the data of both images, the input and the output.
    \sentence
    As such, we can relatively easily loop over every pixel in the original image and compare the color of that pixel
    to the color that is visible at that coordinate within the output SVG image.
    \sentence
    While vector graphics do not use the concept of pixels, we can still go to a specific coordinate on the image and
    query the color visible their using our own means.
    \sentence
    It is important to note here that 100\% may not only be impossible, but in fact is actually not ideal as we are
    not aiming for a truly lossless transformation.
    \sentence
    What we are aiming for is a \italic{Visually Lossless} transformation, meaning any data loss should be invisible
    to the human eye.
    \sentence
    Therefore, while quantitative methods for accuracy are quick and easy, they do not accurately represent the true
    goal of this project, which was never aimed to be lossless.

    \subsubsection{Qualitative Methods}\label{subsubsec:qualitative-methods}

    \tab
    The qualitative methods of comparing accuracy are the most meaningful and important but at the same time, the
    most difficult to measure and make sense of.
    \sentence
    Really, the main judge of whether a compression algorithm is \italic{Visually Lossless} is the human eye, but
    measuring perceived quality and difference has complications.
    \sentence
    However, despite being difficult to accurately measure, as after all, visual perception differs amongst people
    and "quality" can be subjective, there are ways to measure this and make \italic{some} informed conclusion based
    on the data.
    \sentence
    This of course can be done by using a sample set of different people with as diverse possible perception and
    notion of "quality" as possible.
    \sentence
    Despite being very difficult and expensive to gather such a sample set (especially in these times), a sample set
    of close friends and family of fairly diverse ages and backgrounds was gathered and used to measure perceived
    accuracy.

    \subsection{Compression}\label{subsec:compression}

    \tab
    The other property to evaluate is compression rate, i.e., the size ratio of the new file's size to the original
    file's size.
    \sentence
    It is important to note that the project's aim with regard to compression in that the aim is to have smaller file
    sizes \italic{and} scalability, that means there may be instances where file size may actually become larger,
    like when dealing with a low resolution input.
    \sentence
    However, that same output file can now be used on higher resolution displays with no issues, whereas the original
    cannot and any upscaled copies of the original will never be as small in size as our output, at least that is the
    aim.
    \sentence
    Thus, the upscaling factor needs to be taken into account when viewing the data in order to gain a fuller
    understanding of the whole picture.
    \sentence
    Below we detail the many different quantitative methods that can be used to evaluate the compression of the process.

    \subsubsection{Quantitative Methods}\label{subsubsec:quantitative-methods2}

    \tab
    Compression can really only be quantitatively measured but it can be measured fairly easy and therefore, we can
    gather a large amount of data to gain a more concrete understanding of the compression rate of the
    process especially when considering upscaling.
    \sentence
    % TODO 27-Sep-20 Finish this

    \pagebreak


    \section{Results}\label{sec:results}

    \pagebreak


    \section{Challenges}\label{sec:challenges}

    \tab
    As shown by the previous sections in this document, this project has been far from trivial, in fact it was much
    more difficult and challenging than initially anticipated despite already expecting a high level of difficulty.
    \sentence
    While the project has been overall a very mentally stimulating, rewarding and fulfilling project it is important
    to note some of the major challenges that were faced during the development.
    \sentence
    This serves the purpose to inform future readers and those interested in this project about some of the
    challenges faced and how to either overcome them early, or more likely, to expect and prepare for them.
    \sentence
    While some of these issues can be mitigated earlier with further planning and research, the most difficult and
    major challenges are those inherent to the vectorization process and conversion of raster graphics.

    \subsection{Toolchain}\label{subsec:toolchain2}

    \subsubsection{JavaScript}\label{subsubsec:javascript}

    \subsubsection{Concurrency}\label{subsubsec:concurrency}

    \tab
    Arguably the most preventable and frustrating challenges faced were those that were a direct result of the
    toolchain used, especially when there exist other tools that do not have these problems.
    \sentence
    The most frustrating of these is the lack of maturity in the JavaScript ecosystem.
    \sentence
    This can be seen with the very minimal Node.js standard library which contains only the most necessary built in
    functionality and instead delegates anything else to libraries, frameworks and other open source contributors.
    \sentence
    While this does have benefits it also has major disadvantages especially coming from the vast ecosystems of the
    JVM and Android which both come with large fully featured but modular standard libraries that provide standard
    ways to perform common operations.
    \sentence
    This weakness is shown extensively whilst using any JavaScript collections such as Array, Set or Map, which
    provide only the most basic functionality leaving common operations like copying and adding an element to an
    index up to the user or to some external dependency.
    \sentence
    However, perhaps the best example for an argument of the JavaScript ecosystem's immaturity is the fact that
    JavaScript supports object oriented concepts, including classes and methods, but does not allow programmers to
    declare when two objects are equal or not.
    \sentence
    The Set collection contains only distinct elements by first checking if an element already exists before
    inserting it and ignores it if it already exists.
    \sentence
    JavaScript does not allow the programmer to determine how this equality is checked which is done using === by
    default, instead, if the programmer wants a Set of custom objects they have to implement this themselves or
    use an external library, further adding to dependency hell.

    \subsection{Performance}\label{subsec:performance}

    % GPU programming is insanely difficult but yields huge benefits (leading to very difficult decisions)
    % Optimizations to increase speed are also difficult and especially so on a non-multi-threaded environment like Node

    \subsubsection{Hardware Utilization}\label{subsubsec:hardware-utilization}

    % Not using all resources because Node is not multithreaded

    \subsubsection{GPU Programming}\label{subsubsec:gpu-programming}

    \subsection{Minimizing Human Input}\label{subsec:minimizing-human-input}

    % Reducing the number of human inputted arguments to create a fully autonomous deterministic system that is not
    % ML based and produces very accurate results is very difficult

    \pagebreak


    \section{Reflection}\label{sec:reflection}

    % How would you do it differently if to start from scratch (Use JVM or Native and optimize for GPU early)
    % Project Management (Fairly good given the circumstances)
    % So much new knowledge about many different things
    % Very difficult but very mentally stimulating and rewarding project that I really enjoyed

    \pagebreak


    \section{Future Work}\label{sec:future-work}

    \tab
    Despite being given many months to work on this project, it would be dishonest to claim that it is close to being
    completed, as there are still many future endeavours and tasks that can be completed given more time.
    \sentence
    These tasks were left out as they were either lower priority or out of the scope of the project's initial aims
    and objectives and were thus cut for time saving.
    \sentence

    \subsection{Optimization}\label{subsec:optimization}

    \tab
    As discussed in depth in the preceding sections, one of the main issues with the project currently is the
    performance of the processing.
    \sentence
    This was never a high priority to begin with as it was clear early on that in order to aim for high accuracy and
    high compression, more instructions needed to be executed and thus more time was needed to execute them.
    \sentence
    However, given more time this is an angle that is important to investigate and tackle as the more complex the
    process becomes, the longer it takes to view any results, leading to significantly lower productivity.
    \sentence
    One of the main ways this can be achieved is by using the machine's hardware more efficiently especially by using
    more of the resources and making use of the many processing cores most modern machines possess today.
    \sentence
    As discussed earlier, % TODO 25-Sep-20 Node sucks at this and we need multithreading and GPU execution

    \subsection{Future Research}\label{subsec:future-research}

    \tab
    Towards the end of the development cycle many new ideas began to make themselves apparent especially
    after the multiple consecutive failures to show major positive results.
    \sentence
    These ideas were noted and left aside in order to focus on the main goal of the project as they strayed slightly
    from the scope of the project and would have required more than the given time in order to truly explore correctly.
    \sentence
    However, given more time, these ideas should be further explored and tested as they can show further positive
    results and shed some light and insight onto the vectorization problem.
    \sentence
    The most important idea is the potential mixing of raster and vector graphics in a new graphics format in order
    to leverage the best of both formats and attempt to solve the issues of both.
    \sentence
    A potential algorithm would identify easily vectorizable areas of an image that would alter the data of the
    original image only less than a given maximum data loss threshold.
    \sentence
    These areas of the image would be vectorized and represented in vector graphics.
    \sentence
    Then, for the areas where vectorization fails to yield any positive results, usually because of noise,
    rasterization artifacts such as anti-aliasing or other non-vectorizable qualities such as gradients, the
    algorithm would keep those areas as raster pixel-based.
    \sentence
    As is made obvious by the simplistic high-level explanation, this idea is still not well thought out or
    researched, but given more time this is something that can be explored and researched in depth.

    \pagebreak


    \section{Conclusion}\label{sec:conclusion}

    % Summarize everything we said
    % Conclude with final thoughts about the good, the bad and how this can further improve the world etc

    \pagebreak


    \printbibliography[heading=bibintoc,title={References}]

    \pagebreak

\end{document}